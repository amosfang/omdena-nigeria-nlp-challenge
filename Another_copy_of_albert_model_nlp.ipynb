{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hdBQ74ouVl6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#albert model"
      ],
      "metadata": {
        "id": "ngvqxHY06V0w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gavy_eoSuW8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALBERT is a lighter model (approximately 11M parameters) compared to DistilBERT (66M), and while there isn’t a widely available biomedically pre-trained ALBERT-Base on Hugging Face, we’ll fine-tune it downstream on your biomedical data (CSV and PDFs) for disease and treatment prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "UKb4Yj9auerR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install required packages\n",
        "!pip install -q pandas transformers torch langchain pymupdf sentence-transformers faiss-cpu scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anhKJGS-u-4I",
        "outputId": "44a6da45-f4c6-49b4-8dd5-c7d19159065d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community # Install the langchain-community package, which contains the necessary PyMuPDFLoader class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGyUmeFWv8cO",
        "outputId": "fd1e0fe0-02ab-4e5a-9159-9ca02382af00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.40)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2KvLHebu_Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m37PnN9Pu_Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "806S3rFFueJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#optimized\n",
        "Reduced Epochs:\n",
        "Lowered num_train_epochs from 3 to 1. For many tasks, a single epoch can suffice for fine-tuning, especially with a small dataset.\n",
        "\n",
        "Increased Batch Size:\n",
        "Raised per_device_train_batch_size and per_device_eval_batch_size from 16 to 32. ALBERT’s small size (11M parameters) allows larger batches, reducing the number of training steps.\n",
        "\n",
        "Mixed Precision Training (FP16):\n",
        "Added fp16=True in TrainingArguments. This uses half-precision floating-point numbers, speeding up training and reducing memory usage on GPU.\n",
        "\n",
        "Reduced Sequence Length:\n",
        "Lowered max_length from 512 to 256 in prepare_dataset and rag_predict. Shorter sequences decrease computation time, though ensure your symptoms data fits within this limit.\n",
        "\n",
        "Efficient Dataset Preparation:\n",
        "Replaced iterative tokenization with batch tokenization in prepare_dataset. This processes all texts at once, leveraging tokenizer efficiency.\n",
        "\n",
        "Used usecols in pd.read_csv to load only required columns, reducing memory overhead.\n",
        "\n",
        "Multi-threaded PDF Loading:\n",
        "Implemented ThreadPoolExecutor in load_pdfs_from_folder to load PDFs concurrently, speeding up I/O operations.\n",
        "\n",
        "Smaller Text Chunks:\n",
        "Reduced chunk_size from 1000 to 500 and chunk_overlap from 200 to 100 in RecursiveCharacterTextSplitter. Smaller chunks decrease embedding computation time.\n",
        "\n",
        "Reduced Warmup and Logging:\n",
        "Lowered warmup_steps from 200 to 100 and logging_steps from 10 to 5, minimizing overhead during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "7KaTsQNl5kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Paths (adjusted for Colab)\n",
        "DATA_PATH = \"/content/drive/MyDrive/01-nlp/modeling/data/merge_demo_amos_v3.csv\"\n",
        "PDF_FOLDER_PATH = \"/content/drive/MyDrive/01-nlp/modeling/pdf\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/01-nlp/modeling/albert_finetuned\"\n",
        "\n",
        "# Load CSV dataset efficiently\n",
        "df = pd.read_csv(DATA_PATH, usecols=[\"symptoms\", \"disease_name\", \"treatment\"])\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = [\"symptoms\", \"disease_name\", \"treatment\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Column '{col}' is missing from the dataset\")\n",
        "\n",
        "# Load PDFs with multi-threading\n",
        "def load_pdf(file_path):\n",
        "    loader = PyMuPDFLoader(str(file_path))\n",
        "    return loader.load()\n",
        "\n",
        "def load_pdfs_from_folder(folder_path):\n",
        "    pdf_files = list(Path(folder_path).glob(\"*.pdf\"))\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        documents = list(executor.map(load_pdf, pdf_files))\n",
        "    return [doc for sublist in documents for doc in sublist]  # Flatten list\n",
        "\n",
        "pdf_docs = load_pdfs_from_folder(PDF_FOLDER_PATH)\n",
        "print(f\"Loaded {len(pdf_docs)} PDF pages\")\n",
        "\n",
        "# Split PDF documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reduced size for efficiency\n",
        "split_docs = text_splitter.split_documents(pdf_docs)\n",
        "\n",
        "# Create embeddings and vector store for RAG\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
        "\n",
        "# Initialize ALBERT tokenizer and model\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
        "\n",
        "# Prepare dataset efficiently with batch tokenization\n",
        "def prepare_dataset(df, tokenizer, max_length=256):  # Reduced max_length\n",
        "    texts = [f\"Symptoms: {row['symptoms']}\" for _, row in df.iterrows()]\n",
        "    labels = [1 if pd.notna(row[\"treatment\"]) and row[\"treatment\"] != \"\" else 0 for _, row in df.iterrows()]\n",
        "    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": encodings[\"input_ids\"], \"attention_mask\": encodings[\"attention_mask\"]}, labels\n",
        "\n",
        "encodings, labels = prepare_dataset(df, tokenizer)\n",
        "\n",
        "# Convert to PyTorch dataset\n",
        "class DiseaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DiseaseDataset(encodings, labels)\n",
        "\n",
        "# Split dataset for training and evaluation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "eval_size = len(train_dataset) - train_size\n",
        "train_subset, eval_subset = torch.utils.data.random_split(train_dataset, [train_size, eval_size])\n",
        "\n",
        "# Define training arguments (optimized for speed)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/results\",\n",
        "    num_train_epochs=15,  # Reduced epochs\n",
        "    per_device_train_batch_size=32,  # Increased batch size\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,  # Further reduced warmup\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/logs\",\n",
        "    logging_steps=5,  # Reduced logging frequency\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision = precision_score(labels, preds, average=\"binary\")\n",
        "    recall = recall_score(labels, preds, average=\"binary\")\n",
        "    f1 = f1_score(labels, preds, average=\"binary\")\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=eval_subset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "print(\"Starting efficient downstream fine-tuning with ALBERT-Base...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "# Save the fine-tuned model to Google Drive\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# RAG Pipeline: Predict disease and treatment from symptoms\n",
        "def rag_predict(symptoms, vector_store, model, tokenizer, df, top_k=3):\n",
        "    retrieved_docs = vector_store.similarity_search(symptoms, k=top_k)\n",
        "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "    input_text = f\"Symptoms: {symptoms}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=256)  # Match training max_length\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    df[\"symptom_similarity\"] = df[\"symptoms\"].apply(lambda x: 1 if symptoms.lower() in str(x).lower() else 0)\n",
        "    likely_disease_row = df.loc[df[\"symptom_similarity\"].idxmax()]\n",
        "    disease = likely_disease_row[\"disease_name\"]\n",
        "    treatment = likely_disease_row[\"treatment\"] if prediction == 1 and pd.notna(likely_disease_row[\"treatment\"]) else \"No clear treatment identified.\"\n",
        "    return {\n",
        "        \"disease\": disease,\n",
        "        \"treatment\": treatment,\n",
        "        \"context_snippet\": context[:200] + \"...\" if len(context) > 200 else context\n",
        "    }\n",
        "# Example usage\n",
        "symptoms_query = \"fever, cough, fatigue\"\n",
        "prediction = rag_predict(symptoms_query, vector_store, model, tokenizer, df)\n",
        "print(\"\\nPrediction Result:\")\n",
        "print(f\"Disease: {prediction['disease']}\")\n",
        "print(f\"Treatment: {prediction['treatment']}\")\n",
        "print(f\"Context from PDFs: {prediction['context_snippet']}\")\n",
        "\n",
        "# Print model evaluation summary\n",
        "print(\"\\nModel Evaluation Summary:\")\n",
        "for metric, value in eval_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qcnFcR0fuXC_",
        "outputId": "3efb8f09-17c4-4c22-b1da-e48c3c665e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5338 PDF pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting efficient downstream fine-tuning with ALBERT-Base...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2835' max='2835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2835/2835 25:36, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.433100</td>\n",
              "      <td>0.407003</td>\n",
              "      <td>0.732877</td>\n",
              "      <td>0.757075</td>\n",
              "      <td>0.744780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.498800</td>\n",
              "      <td>0.433561</td>\n",
              "      <td>0.801325</td>\n",
              "      <td>0.570755</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.471400</td>\n",
              "      <td>0.391940</td>\n",
              "      <td>0.719828</td>\n",
              "      <td>0.787736</td>\n",
              "      <td>0.752252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.418300</td>\n",
              "      <td>0.398073</td>\n",
              "      <td>0.729490</td>\n",
              "      <td>0.775943</td>\n",
              "      <td>0.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.352600</td>\n",
              "      <td>0.394021</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.700472</td>\n",
              "      <td>0.734240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.349500</td>\n",
              "      <td>0.391105</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.759434</td>\n",
              "      <td>0.750583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.362600</td>\n",
              "      <td>0.383133</td>\n",
              "      <td>0.777228</td>\n",
              "      <td>0.740566</td>\n",
              "      <td>0.758454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.428200</td>\n",
              "      <td>0.432785</td>\n",
              "      <td>0.822695</td>\n",
              "      <td>0.547170</td>\n",
              "      <td>0.657224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.350300</td>\n",
              "      <td>0.394015</td>\n",
              "      <td>0.763959</td>\n",
              "      <td>0.709906</td>\n",
              "      <td>0.735941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.291500</td>\n",
              "      <td>0.382192</td>\n",
              "      <td>0.743056</td>\n",
              "      <td>0.757075</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.375731</td>\n",
              "      <td>0.743182</td>\n",
              "      <td>0.771226</td>\n",
              "      <td>0.756944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.278800</td>\n",
              "      <td>0.384346</td>\n",
              "      <td>0.769048</td>\n",
              "      <td>0.761792</td>\n",
              "      <td>0.765403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.393503</td>\n",
              "      <td>0.793367</td>\n",
              "      <td>0.733491</td>\n",
              "      <td>0.762255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.348600</td>\n",
              "      <td>0.386724</td>\n",
              "      <td>0.786600</td>\n",
              "      <td>0.747642</td>\n",
              "      <td>0.766626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.436900</td>\n",
              "      <td>0.390869</td>\n",
              "      <td>0.791457</td>\n",
              "      <td>0.742925</td>\n",
              "      <td>0.766423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.3757314383983612, 'eval_precision': 0.7431818181818182, 'eval_recall': 0.7712264150943396, 'eval_f1': 0.7569444444444444, 'eval_runtime': 8.6643, 'eval_samples_per_second': 174.163, 'eval_steps_per_second': 5.54, 'epoch': 15.0}\n",
            "Model saved to /content/drive/MyDrive/01-nlp/modeling/albert_finetuned\n",
            "\n",
            "Prediction Result:\n",
            "Disease: 5 common symptoms of ovulation\n",
            "Treatment: No clear treatment identified.\n",
            "Context from PDFs: Symptoms: Fever, fatigue, muscle aches, runny nose.   \n",
            "Diagnosis: Clinical symptoms, viral PCR if needed, rule out bacterial.   \n",
            "Treatment: Supportive care, acetaminophen (650 mg), fluids. \n",
            "Pneumonic ...\n",
            "\n",
            "Model Evaluation Summary:\n",
            "eval_loss: 0.3757\n",
            "eval_precision: 0.7432\n",
            "eval_recall: 0.7712\n",
            "eval_f1: 0.7569\n",
            "eval_runtime: 8.6643\n",
            "eval_samples_per_second: 174.1630\n",
            "eval_steps_per_second: 5.5400\n",
            "epoch: 15.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another usage\n",
        "symptoms_query = \"Fatigue, right upper quadrant discomfort\"\n",
        "prediction = rag_predict(symptoms_query, vector_store, model, tokenizer, df)\n",
        "print(\"\\nPrediction Result:\")\n",
        "print(f\"Disease: {prediction['disease']}\")\n",
        "print(f\"Treatment: {prediction['treatment']}\")\n",
        "print(f\"Context from PDFs: {prediction['context_snippet']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JTLUs7EILG4",
        "outputId": "92c4e164-ca1c-49d5-c091-2fe2a3e23cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction Result:\n",
            "Disease: 5 common symptoms of ovulation\n",
            "Treatment: No clear treatment identified.\n",
            "Context from PDFs: - Symptoms: Fatigue, right upper quadrant discomfort, enlarged liver. \n",
            "- Diagnosis: Ultrasound (fatty liver), LFTs (AST >ALT), history. \n",
            "- Treatment: Abstinence, vitamin E (800 IU daily), nutritional ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model evaluation summary\n",
        "print(\"\\nModel Evaluation Summary:\")\n",
        "for metric, value in eval_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a0T5o2sCyDd",
        "outputId": "79e733ce-51c8-45b1-f4d0-6fe0451ad1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Summary:\n",
            "eval_loss: 0.3617\n",
            "eval_precision: 0.8122\n",
            "eval_recall: 0.7483\n",
            "eval_f1: 0.7789\n",
            "eval_runtime: 8.7789\n",
            "eval_samples_per_second: 171.8900\n",
            "eval_steps_per_second: 5.4680\n",
            "epoch: 5.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0LAPk9YCyHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#streamlit code"
      ],
      "metadata": {
        "id": "fmGptN6n-z5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
        "import torch\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Paths (adjust these paths based on where your files are stored locally or in your environment)\n",
        "DATA_PATH = \"merge_demo_amos_v3.csv\"  # Update to your local path\n",
        "PDF_FOLDER_PATH = \"pdf\"  # Update to your local path\n",
        "MODEL_SAVE_PATH = \"albert_finetuned\"  # Update to your local path\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "    model = AlbertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return tokenizer, model\n",
        "\n",
        "# Load CSV dataset\n",
        "@st.cache_data\n",
        "def load_csv_data():\n",
        "    df = pd.read_csv(DATA_PATH, usecols=[\"symptoms\", \"disease_name\", \"treatment\"])\n",
        "    return df\n",
        "\n",
        "# Load and process PDFs for RAG\n",
        "@st.cache_resource\n",
        "def load_vector_store():\n",
        "    def load_pdf(file_path):\n",
        "        loader = PyMuPDFLoader(str(file_path))\n",
        "        return loader.load()\n",
        "\n",
        "    def load_pdfs_from_folder(folder_path):\n",
        "        pdf_files = list(Path(folder_path).glob(\"*.pdf\"))\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            documents = list(executor.map(load_pdf, pdf_files))\n",
        "        return [doc for sublist in documents for doc in sublist]\n",
        "\n",
        "    pdf_docs = load_pdfs_from_folder(PDF_FOLDER_PATH)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    split_docs = text_splitter.split_documents(pdf_docs)\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
        "    return vector_store\n",
        "\n",
        "# Prediction function (adapted from your rag_predict)\n",
        "def rag_predict(symptoms, vector_store, model, tokenizer, df, top_k=3):\n",
        "    retrieved_docs = vector_store.similarity_search(symptoms, k=top_k)\n",
        "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "    input_text = f\"Symptoms: {symptoms}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    df[\"symptom_similarity\"] = df[\"symptoms\"].apply(lambda x: 1 if symptoms.lower() in str(x).lower() else 0)\n",
        "    likely_disease_row = df.loc[df[\"symptom_similarity\"].idxmax()]\n",
        "    disease = likely_disease_row[\"disease_name\"]\n",
        "    treatment = likely_disease_row[\"treatment\"] if prediction == 1 and pd.notna(likely_disease_row[\"treatment\"]) else \"No clear treatment identified.\"\n",
        "    return {\n",
        "        \"disease\": disease,\n",
        "        \"treatment\": treatment,\n",
        "        \"context_snippet\": context[:200] + \"...\" if len(context) > 200 else context\n",
        "    }\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Symptom-Based Disease and Treatment Prediction\")\n",
        "    st.write(\"Enter your symptoms below to predict the likely disease and treatment based on a fine-tuned ALBERT model and RAG pipeline.\")\n",
        "\n",
        "    # Load resources\n",
        "    with st.spinner(\"Loading model and data...\"):\n",
        "        tokenizer, model = load_model_and_tokenizer()\n",
        "        df = load_csv_data()\n",
        "        vector_store = load_vector_store()\n",
        "\n",
        "    # User input\n",
        "    symptoms = st.text_area(\"Enter your symptoms (e.g., fever, cough, fatigue):\", \"\")\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if symptoms.strip() == \"\":\n",
        "            st.error(\"Please enter some symptoms.\")\n",
        "        else:\n",
        "            with st.spinner(\"Making prediction...\"):\n",
        "                prediction = rag_predict(symptoms, vector_store, model, tokenizer, df)\n",
        "                st.success(\"Prediction complete!\")\n",
        "\n",
        "                # Display results\n",
        "                st.subheader(\"Prediction Results\")\n",
        "                st.write(f\"**Likely Disease:** {prediction['disease']}\")\n",
        "                st.write(f\"**Recommended Treatment:** {prediction['treatment']}\")\n",
        "                st.write(f\"**Context from PDFs:** {prediction['context_snippet']}\")\n",
        "\n",
        "    # Footer\n",
        "    st.write(\"---\")\n",
        "    st.write(\"Built with Streamlit, ALBERT, and LangChain by xAI.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "nSPms1Kb-12s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8WgjnSCuXGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#albert model"
      ],
      "metadata": {
        "id": "0CqrqVRMERPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Install required packages\n",
        "!pip install -q pandas transformers torch langchain pymupdf sentence-transformers faiss-cpu scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths (adjusted for Colab)\n",
        "DATA_PATH = \"/content/drive/MyDrive/01-nlp/modeling/data/merge_demo_amos_v3.csv\"  # Your CSV path\n",
        "PDF_FOLDER_PATH = \"/content/drive/MyDrive/01-nlp/modeling/pdf\"  # Your PDF folder path\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/01-nlp/modeling/albert_finetuned\"  # Save path for model\n",
        "\n",
        "# Load CSV dataset\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = [\"symptoms\", \"disease_name\", \"treatment\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Column '{col}' is missing from the dataset\")\n",
        "\n",
        "# Load PDFs from folder\n",
        "def load_pdfs_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for pdf_file in Path(folder_path).glob(\"*.pdf\"):\n",
        "        loader = PyMuPDFLoader(str(pdf_file))\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "pdf_docs = load_pdfs_from_folder(PDF_FOLDER_PATH)\n",
        "print(f\"Loaded {len(pdf_docs)} PDF pages\")\n",
        "\n",
        "# Split PDF documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_docs = text_splitter.split_documents(pdf_docs)\n",
        "\n",
        "# Create embeddings and vector store for RAG\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
        "\n",
        "# Initialize ALBERT tokenizer and model\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)  # Binary classification\n",
        "\n",
        "# Prepare dataset for downstream fine-tuning\n",
        "def prepare_dataset(df):\n",
        "    encodings = {'input_ids': [], 'attention_mask': []}\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = f\"Symptoms: {row['symptoms']}\"  # Input is symptoms only\n",
        "        encoding = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "        # Label: 1 if treatment exists, 0 if not\n",
        "        label = 1 if pd.notna(row[\"treatment\"]) and row[\"treatment\"] != \"\" else 0\n",
        "        encodings['input_ids'].append(encoding[\"input_ids\"].squeeze())\n",
        "        encodings['attention_mask'].append(encoding[\"attention_mask\"].squeeze())\n",
        "        labels.append(label)\n",
        "    return encodings, labels\n",
        "\n",
        "encodings, labels = prepare_dataset(df)\n",
        "\n",
        "# Convert to PyTorch dataset\n",
        "class DiseaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DiseaseDataset(encodings, labels)\n",
        "\n",
        "# Split dataset for training and evaluation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "eval_size = len(train_dataset) - train_size\n",
        "train_subset, eval_subset = torch.utils.data.random_split(train_dataset, [train_size, eval_size])\n",
        "\n",
        "# Define training arguments (optimized for ALBERT's smaller size)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/results\",  # Temp dir in Colab\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,  # Larger batch size due to ALBERT's efficiency\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=200,  # Reduced warmup steps for faster convergence\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/logs\",  # Temp dir in Colab\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"  # Disable wandb logging in Colab\n",
        ")\n",
        "\n",
        "# Define metrics for evaluation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision = precision_score(labels, preds, average=\"binary\")\n",
        "    recall = recall_score(labels, preds, average=\"binary\")\n",
        "    f1 = f1_score(labels, preds, average=\"binary\")\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=eval_subset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Fine-tune the model downstream\n",
        "print(\"Starting downstream fine-tuning with ALBERT-Base...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "# Save the fine-tuned model to Google Drive\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# RAG Pipeline: Predict disease and treatment from symptoms\n",
        "def rag_predict(symptoms, vector_store, model, tokenizer, df, top_k=3):\n",
        "    # Retrieve relevant documents from vector store\n",
        "    retrieved_docs = vector_store.similarity_search(symptoms, k=top_k)\n",
        "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Tokenize input symptoms\n",
        "    input_text = f\"Symptoms: {symptoms}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    # Predict treatment presence\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    # Find closest matching disease from CSV based on symptoms\n",
        "    df[\"symptom_similarity\"] = df[\"symptoms\"].apply(lambda x: 1 if symptoms.lower() in str(x).lower() else 0)\n",
        "    likely_disease_row = df.loc[df[\"symptom_similarity\"].idxmax()]\n",
        "    disease = likely_disease_row[\"disease_name\"]\n",
        "    treatment = likely_disease_row[\"treatment\"] if prediction == 1 and pd.notna(likely_disease_row[\"treatment\"]) else \"No clear treatment identified.\"\n",
        "\n",
        "    # Generate response\n",
        "    response = {\n",
        "        \"disease\": disease,\n",
        "        \"treatment\": treatment,\n",
        "        \"context_snippet\": context[:200] + \"...\" if len(context) > 200 else context\n",
        "    }\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "symptoms_query = \"fever, cough, fatigue\"\n",
        "prediction = rag_predict(symptoms_query, vector_store, model, tokenizer, df)\n",
        "print(\"\\nPrediction Result:\")\n",
        "print(f\"Disease: {prediction['disease']}\")\n",
        "print(f\"Treatment: {prediction['treatment']}\")\n",
        "print(f\"Context from PDFs: {prediction['context_snippet']}\")\n",
        "\n",
        "# Print model evaluation summary\n",
        "print(\"\\nModel Evaluation Summary:\")\n",
        "for metric, value in eval_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "n2B8ua3UETql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}